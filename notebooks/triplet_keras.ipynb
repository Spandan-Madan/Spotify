{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations in Keras using triplet loss\n",
    "Along the lines of BPR [1]. \n",
    "\n",
    "[1] Rendle, Steffen, et al. \"BPR: Bayesian personalized ranking from implicit feedback.\" Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009.\n",
    "\n",
    "This is implemented (more efficiently) in LightFM (https://github.com/lyst/lightfm). See the MovieLens example (https://github.com/lyst/lightfm/blob/master/examples/movielens/example.ipynb) for results comparable to this notebook.\n",
    "\n",
    "## Set up the architecture\n",
    "A simple dense layer for both users and items: this is exactly equivalent to latent factor matrix when multiplied by binary user and item indices. There are three inputs: users, positive items, and negative items. In the triplet objective we try to make the positive item rank higher than the negative item for that user.\n",
    "\n",
    "Because we want just one single embedding for the items, we use shared weights for the positive and negative item inputs (a siamese architecture).\n",
    "\n",
    "This is all very simple but could be made arbitrarily complex, with more layers, conv layers and so on. I expect we'll be seeing a lot of papers doing just that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Triplet loss network example for recommenders\n",
    "\"\"\"\n",
    "from keras.models import load_model\n",
    "from __future__ import print_function\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, merge\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense,Dropout\n",
    "\n",
    "import data\n",
    "import metrics\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "def bpr_triplet_loss(X):\n",
    "    first_item_latent, second_item_latent, random_item_latent = X\n",
    "\n",
    "    # BPR loss\n",
    "    loss = 1.0 - K.sigmoid(\n",
    "        K.sum(first_item_latent * second_item_latent, axis=-1, keepdims=True) -\n",
    "        K.sum(first_item_latent * random_item_latent, axis=-1, keepdims=True))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def standard_triplet_loss(X):\n",
    "    first_item_latent, random_item_latent, second_item_latent = X\n",
    "    \n",
    "    term1 = K.pow((first_item_latent - second_item_latent),2)\n",
    "    sum1 = K.sum(term1)\n",
    "    term2 = K.pow((first_item_latent - random_item_latent),2)\n",
    "    sum2 = K.sum(term2)\n",
    "    loss = K.maximum(sum1 - sum2 + 0.2,0)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform data\n",
    "We're going to load the Movielens 100k dataset and create triplets of (user, known positive item, randomly sampled negative item).\n",
    "\n",
    "The success metric is AUC: in this case, the probability that a randomly chosen known positive item from the test set is ranked higher for a given user than a ranomly chosen negative item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading songs and converting into one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = []\n",
    "for pid in range(1428):\n",
    "    f = open('/Users/spandanmadan/Desktop/Spotify/Spotify/data/pooling/scraped_lyrics_%s.p'%pid,'rb')\n",
    "    lyrics_list = pickle.load(f)\n",
    "    f.close()\n",
    "    all_lyrics += lyrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=0.01,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [' '.join(words) for words in all_lyrics]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(max_df = 0.9,min_df = 0.01)\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uris = []\n",
    "pid_to_uris = {}\n",
    "for pid in range(1428):\n",
    "    f = open('/Users/spandanmadan/Desktop/Spotify/Spotify/data/pooling/scraped_lyrics_uris_%s.p'%pid,'rb')\n",
    "    uris_list = pickle.load(f)\n",
    "    f.close()\n",
    "    pid_to_uris[pid] = uris_list\n",
    "    all_uris += uris_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65781"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform(all_lyrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(562, 1404)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n"
     ]
    }
   ],
   "source": [
    "uri_to_lyrics = {}\n",
    "for i in range(len(all_uris)):\n",
    "    if i %1000 == 0:\n",
    "        print(i)\n",
    "    uri = all_uris[i]\n",
    "    lyrics = all_lyrics[i]\n",
    "    if lyrics == []:\n",
    "        lyrics = [' ']\n",
    "#     vector = vectorizer.transform(lyrics)\n",
    "#     nparray = vector.toarray()\n",
    "    uri_to_lyrics[uri] = [' '.join(lyrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('uri_to_lyrics.p','wb')\n",
    "pickle.dump(uri_to_lyrics,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = vectorizer.transform(uri_to_lyrics['0UaMYEvWZi0ZqiDOoHU3YI']).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uris = list(set(uri_to_lyrics.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1404)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(uri_to_lyrics['0UaMYEvWZi0ZqiDOoHU3YI']).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_batch(BATCH_SIZE,vector_size,unique_uris):\n",
    "    found = 0\n",
    "    while not found:\n",
    "        pid = random.randint(0,1427)\n",
    "        playlist_uris = pid_to_uris[pid]\n",
    "        \n",
    "        first_part = playlist_uris[:int(len(playlist_uris)*0.75)]\n",
    "        second_part = playlist_uris[int(len(playlist_uris)*0.75):]\n",
    "        random_part = [i for i in unique_uris if i not in playlist_uris]\n",
    "        \n",
    "        try:\n",
    "            indices1 = random.sample(range(1,len(first_part)),BATCH_SIZE)\n",
    "            indices2 = random.sample(range(1,len(second_part)),BATCH_SIZE)\n",
    "            indices3 = random.sample(range(1,len(random_part)),BATCH_SIZE)\n",
    "            found =1 \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    data_first_song = np.zeros((BATCH_SIZE,vector_size))\n",
    "    data_second_song = np.zeros((BATCH_SIZE,vector_size))\n",
    "    data_random_song = np.zeros((BATCH_SIZE,vector_size))\n",
    "    \n",
    "    first_uris = []\n",
    "    second_uris = []\n",
    "    random_uris = []\n",
    "    \n",
    "    for i in range(len(indices1)):\n",
    "        data_first_song[i] = vectorizer.transform(uri_to_lyrics[first_part[indices1[i]]]).toarray()\n",
    "        data_second_song[i] = vectorizer.transform(uri_to_lyrics[second_part[indices2[i]]]).toarray()\n",
    "        data_random_song[i] = vectorizer.transform(uri_to_lyrics[random_part[indices3[i]]]).toarray()\n",
    "        \n",
    "        first_uris.append(first_part[indices1[i]])\n",
    "        second_uris.append(second_part[indices2[i]])\n",
    "        random_uris.append(random_part[indices3[i]])\n",
    "        \n",
    "    uris = (first_uris,second_uris,random_uris)\n",
    "    reshaped_1 = data_first_song.reshape((BATCH_SIZE,12,-1))\n",
    "    reshaped_2 = data_second_song.reshape((BATCH_SIZE,12,-1))\n",
    "    reshaped_random = data_random_song.reshape((BATCH_SIZE,12,-1))\n",
    "    return reshaped_1, reshaped_2, reshaped_random,uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 300\n",
    "NUM_EPOCHS = 3000\n",
    "BATCH_SIZE = 5\n",
    "timesteps = 12\n",
    "features = 117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = get_train_batch(BATCH_SIZE,vector_size,unique_uris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visible = Input(shape=(100,1))\n",
    "# # feature extraction\n",
    "# extract = LSTM(10, return_sequences=True)(visible)\n",
    "# # classification output\n",
    "# class11 = LSTM(10)(extract)\n",
    "# class12 = Dense(10, activation='relu')(class11)\n",
    "# output1 = Dense(1, activation='sigmoid')(class12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_merge(X):\n",
    "    first,second,random = X\n",
    "    good = K.pow((first-second),2)\n",
    "    bad = K.pow((first-random),2)\n",
    "    \n",
    "    K.sum(good,bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"lo...)`\n"
     ]
    }
   ],
   "source": [
    "positive_item_input = Input((1, ), name='positive_item_input')\n",
    "negative_item_input = Input((1, ), name='negative_item_input')\n",
    "\n",
    "# Shared embedding layer for positive and negative items\n",
    "item_embedding_layer = Embedding(\n",
    "    10, 10, name='item_embedding', input_length=1)\n",
    "\n",
    "user_input = Input((1, ), name='user_input')\n",
    "\n",
    "positive_item_embedding = Flatten()(item_embedding_layer(\n",
    "    positive_item_input))\n",
    "negative_item_embedding = Flatten()(item_embedding_layer(\n",
    "    negative_item_input))\n",
    "user_embedding = Flatten()(Embedding(\n",
    "    10, 10, name='user_embedding', input_length=1)(\n",
    "        user_input))\n",
    "\n",
    "loss = merge(\n",
    "    [positive_item_embedding, negative_item_embedding, user_embedding],\n",
    "    mode=bpr_triplet_loss,\n",
    "    name='loss',\n",
    "    output_shape=(1, ))\n",
    "\n",
    "model__ = Model(\n",
    "    input=[positive_item_input, negative_item_input, user_input],\n",
    "    output=loss)\n",
    "model__.compile(loss=identity_loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_LSTM_layer = LSTM(700,return_sequences=True)\n",
    "shared_dropout_layer = Dropout(0.5)\n",
    "shared_LSTM_layer2 = LSTM(500,return_sequences=True)\n",
    "shared_dense_layer = Dense(1000,activation='relu')\n",
    "shared_dense_layer_2 = Dense(300, activation='relu')\n",
    "shared_dropout_layer_2 = Dropout(0.5)\n",
    "shared_flatten = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared_dense_layer_2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "# first input\n",
    "visible_1 = Input(shape=(timesteps,features))\n",
    "extract_1 = shared_LSTM_layer(visible_1)\n",
    "dropped_1 = shared_dropout_layer(extract_1)\n",
    "extract_2 = shared_LSTM_layer2(dropped_1)\n",
    "dropped_2 = shared_dropout_layer_2(extract_2)\n",
    "flatten_1 = shared_flatten(dropped_2)\n",
    "dense_1 = shared_dense_layer(flatten_1)\n",
    "dense_2 = shared_dense_layer_2(dense_1)\n",
    "\n",
    "# Second input\n",
    "visible_1_2 = Input(shape=(timesteps,features))\n",
    "extract_1_2 = shared_LSTM_layer(visible_1_2)\n",
    "dropped_1_2 = shared_dropout_layer(extract_1_2)\n",
    "extract_2_2 = shared_LSTM_layer2(dropped_1_2)\n",
    "dropped_2_2 = shared_dropout_layer_2(extract_2_2)\n",
    "flatten_1_2 = shared_flatten(dropped_2_2)\n",
    "dense_1_2 = shared_dense_layer(flatten_1_2)\n",
    "dense_2_2 = shared_dense_layer_2(dense_1_2)\n",
    "\n",
    "# Third input \n",
    "visible_1_3 = Input(shape=(timesteps,features))\n",
    "extract_1_3 = shared_LSTM_layer(visible_1_3)\n",
    "dropped_1_3 = shared_dropout_layer(extract_1_3)\n",
    "extract_2_3 = shared_LSTM_layer2(dropped_1_3)\n",
    "dropped_2_3 = shared_dropout_layer_2(extract_2_3)\n",
    "flatten_1_3 = shared_flatten(dropped_2_3)\n",
    "dense_1_3 = shared_dense_layer(flatten_1_3)\n",
    "dense_2_3 = shared_dense_layer_2(dense_1_3)\n",
    "\n",
    "output = merge([dense_2,dense_2_2,dense_2_3],mode=standard_triplet_loss,output_shape=(1,))\n",
    "model = Model(inputs=[visible_1,visible_1_2,visible_1_3], outputs=output)\n",
    "plot_model(model,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 12, 117)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 12, 117)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 12, 117)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 12, 700)      2290400     input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 12, 700)      0           lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 12, 500)      2402000     dropout_1[3][0]                  \n",
      "                                                                 dropout_1[4][0]                  \n",
      "                                                                 dropout_1[5][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 12, 500)      0           lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 6000)         0           dropout_2[3][0]                  \n",
      "                                                                 dropout_2[4][0]                  \n",
      "                                                                 dropout_2[5][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         6001000     flatten_4[3][0]                  \n",
      "                                                                 flatten_4[4][0]                  \n",
      "                                                                 flatten_4[5][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 300)          300300      dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 1)            0           dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,993,700\n",
      "Trainable params: 10,993,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared_LSTM_layer = LSTM(700,return_sequences=True)\n",
    "# shared_dropout_layer = Dropout(0.5)\n",
    "# shared_LSTM_layer2 = LSTM(500,return_sequences=True)\n",
    "# shared_dense_layer = Dense(500,activation='relu')\n",
    "# shared_dense_layer_2 = Dense(300, activation='relu')\n",
    "\n",
    "# # first input\n",
    "# visible_1 = Input(shape=(timesteps,features))\n",
    "# extract_1 = shared_LSTM_layer(visible_1)\n",
    "# dropped_1 = shared_dropout_layer(extract_1)\n",
    "# extract_2 = shared_LSTM_layer2(dropped_1)\n",
    "# dropped_2 = shared_dropout_layer(extract_2)\n",
    "# dense_1 = shared_dense_layer(dropped_2)\n",
    "# dense_2 = shared_dense_layer_2(dense_1)\n",
    "\n",
    "# # Second input\n",
    "# visible_1_2 = Input(shape=(timesteps,features))\n",
    "# extract_1_2 = shared_LSTM_layer(visible_1_2)\n",
    "# dropped_1_2 = shared_dropout_layer(extract_1_2)\n",
    "# extract_2_2 = shared_LSTM_layer2(dropped_1_2)\n",
    "# dropped_2_2 = shared_dropout_layer(extract_2_2)\n",
    "# dense_1_2 = shared_dense_layer(dropped_2_2)\n",
    "# dense_2_2 = shared_dense_layer_2(dense_1_2)\n",
    "\n",
    "\n",
    "# # Third input\n",
    "# visible_1_3 = Input(shape=(timesteps,features))\n",
    "# extract_1_3 = shared_LSTM_layer(visible_1_3)\n",
    "# dropped_1_3 = shared_dropout_layer(extract_1_3)\n",
    "# extract_2_3 = shared_LSTM_layer2(dropped_1_3)\n",
    "# dropped_2_3 = shared_dropout_layer(extract_2_3)\n",
    "# dense_1_3 = shared_dense_layer(dropped_2_3)\n",
    "# dense_2_3 = shared_dense_layer_2(dense_1_3)\n",
    "\n",
    "# # merge(\n",
    "# #         [positive_item_embedding, negative_item_embedding, user_embedding],\n",
    "# #         mode=bpr_triplet_loss,\n",
    "# #         name='loss',\n",
    "# #         output_shape=(1, ))\n",
    "# output = merge([visible_1,visible_1_2,visible_1_3],mode=standard_triplet_loss,output_shape=(1,))\n",
    "# model = Model(inputs=[visible_1, visible_1_2,visible_1_3], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=identity_loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "Run for a couple of epochs, checking the AUC after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Epoch 100\n",
      "Epoch 150\n",
      "Epoch 200\n",
      "Epoch 250\n",
      "Epoch 300\n",
      "Epoch 350\n",
      "Epoch 400\n",
      "Epoch 450\n",
      "Epoch 500\n",
      "Epoch 550\n",
      "Epoch 600\n",
      "Epoch 650\n",
      "Epoch 700\n",
      "Epoch 750\n",
      "Epoch 800\n",
      "Epoch 850\n",
      "Epoch 900\n",
      "Epoch 950\n",
      "Epoch 1000\n",
      "Epoch 1050\n",
      "Epoch 1100\n",
      "Epoch 1150\n",
      "Epoch 1200\n",
      "Epoch 1250\n",
      "Epoch 1300\n",
      "Epoch 1350\n",
      "Epoch 1400\n",
      "Epoch 1450\n",
      "Epoch 1500\n",
      "Epoch 1550\n",
      "Epoch 1600\n",
      "Epoch 1650\n",
      "Epoch 1700\n",
      "Epoch 1750\n",
      "Epoch 1800\n",
      "Epoch 1850\n",
      "Epoch 1900\n",
      "Epoch 1950\n",
      "Epoch 2000\n",
      "Epoch 2050\n",
      "Epoch 2100\n",
      "Epoch 2150\n",
      "Epoch 2200\n",
      "Epoch 2250\n",
      "Epoch 2300\n",
      "Epoch 2350\n",
      "Epoch 2400\n",
      "Epoch 2450\n",
      "Epoch 2500\n",
      "Epoch 2550\n",
      "Epoch 2600\n",
      "Epoch 2650\n",
      "Epoch 2700\n",
      "Epoch 2750\n",
      "Epoch 2800\n",
      "Epoch 2850\n",
      "Epoch 2900\n",
      "Epoch 2950\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    if epoch % 50 == 0:\n",
    "        print('Epoch %s' % epoch)\n",
    "    \n",
    "    # Sample triplets from the training data\n",
    "    first_data, second_data, random_data,uris = get_train_batch(BATCH_SIZE,vector_size,unique_uris)\n",
    "    X = [first_data,second_data,random_data]\n",
    "    model.fit(X,\n",
    "              np.ones(len(first_data)),\n",
    "              batch_size=BATCH_SIZE,\n",
    "              nb_epoch=1,\n",
    "              verbose=0,\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = '/Users/spandanmadan/Desktop/Spotify/Spotify/data/pooling/saved_lstm.h5'\n",
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(timesteps,features))\n",
    "extract = shared_LSTM_layer(visible)\n",
    "drop = shared_dropout_layer(extract)\n",
    "extract_ = shared_LSTM_layer2(drop)\n",
    "drop_ = shared_dropout_layer_2(extract_)\n",
    "flat = shared_flatten(drop_)\n",
    "dense = shared_dense_layer(flat)\n",
    "dense_ = shared_dense_layer_2(dense)\n",
    "\n",
    "# model = Model(inputs=[visible_1, visible_1_2,visible_1_3], outputs=output)\n",
    "prediction_model = Model(inputs=visible,outputs=dense_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = '/Users/spandanmadan/Desktop/Spotify/Spotify/data/pooling/saved_prediction_model.h5'\n",
    "prediction_model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_model = Sequential()\n",
    "# prediction_model.add(LSTM(700,return_sequences=True,\n",
    "#                input_shape=(timesteps, features),weights=shared_LSTM_layer.get_weights()))  # returns a sequence of vectors of dimension 32\n",
    "# prediction_model.add(Dropout(0.5))\n",
    "# prediction_model.add(LSTM(500, return_sequences=True,weights=shared_LSTM_layer2.get_weights()))  # returns a sequence of vectors of dimension 32\n",
    "# prediction_model.add(Dropout(0.5))\n",
    "# prediction_model.add(Dense(500, activation='relu',weights=shared_dense_layer.get_weights()))\n",
    "# prediction_model.add(Dense(300, activation='relu',weights=shared_dense_layer_2.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(prediction_model,to_file='prediction_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 12, 117)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12, 700)           2290400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 700)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 12, 500)           2402000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 500)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              6001000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               300300    \n",
      "=================================================================\n",
      "Total params: 10,993,700\n",
      "Trainable params: 10,993,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 12, 117)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 12, 117)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 12, 117)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 12, 700)      2290400     input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 12, 700)      0           lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 12, 500)      2402000     dropout_1[3][0]                  \n",
      "                                                                 dropout_1[4][0]                  \n",
      "                                                                 dropout_1[5][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 12, 500)      0           lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 6000)         0           dropout_2[3][0]                  \n",
      "                                                                 dropout_2[4][0]                  \n",
      "                                                                 dropout_2[5][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         6001000     flatten_4[3][0]                  \n",
      "                                                                 flatten_4[4][0]                  \n",
      "                                                                 flatten_4[5][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 300)          300300      dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 1)            0           dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,993,700\n",
      "Trainable params: 10,993,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_songs(uri_list):\n",
    "    for uri in uri_list:\n",
    "        print(uri_to_name_artist['spotify:track:'+uri])\n",
    "\n",
    "def compare_outputs(activations_first,activations_second,activations_random):\n",
    "    mean_1 = np.mean(activations_first,axis=0)\n",
    "    mean_2 = np.mean(activations_second,axis=0)\n",
    "    mean_random = np.mean(activations_random,axis=0)\n",
    "    \n",
    "    A = np.linalg.norm(mean_1 - mean_2,2)\n",
    "    B = np.linalg.norm(mean_1 - mean_random,2)\n",
    "    \n",
    "    return A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = get_train_batch(BATCH_SIZE,vector_size,unique_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_first = prediction_model.predict(a)\n",
    "activations_second = prediction_model.predict(b)\n",
    "activations_random = prediction_model.predict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/Users/spandanmadan/Desktop/Spotify/Spotify/data/pooling/uri_to_name_artist.p','rb')\n",
    "uri_to_name_artist = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First songs-\n",
      "('An Anthem of Invitation', 'Judah & the Lion')\n",
      "('Shepherd of My Soul', 'Rivers & Robots')\n",
      "('Fall Down', 'Rivers & Robots')\n",
      "('I Surrender All', 'Ascend The Hill')\n",
      "('Oh Love That Will Not Let Me Go', 'Ascend The Hill')\n",
      "Second songs-\n",
      "('Voice That Stills the Raging Sea', 'Rivers & Robots')\n",
      "('Light Will Dawn', 'Rivers & Robots')\n",
      "('See the Way (feat. David Brymer)', 'Misty Edwards')\n",
      "('Shepherd of My Soul', 'Rivers & Robots')\n",
      "('Farther Along', 'Josh Garrels')\n",
      "Third songs-\n",
      "('Everybody Wants to Go to Heaven', 'Kenny Chesney')\n",
      "('All I Ask Of You (feat. Pennybirdrabbit) - feat. Penny', 'Skrillex')\n",
      "('Jorge Regula', 'The Moldy Peaches')\n",
      "('Dawn of Time', 'Tribal Seeds')\n",
      "('Never Too Far Gone', 'Jordan Feliz')\n"
     ]
    }
   ],
   "source": [
    "print('First songs-')\n",
    "print_songs(d[0])\n",
    "print('Second songs-')\n",
    "print_songs(d[1])\n",
    "print('Third songs-')\n",
    "print_songs(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.009273529, 0.016392708)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_outputs(activations_first,activations_second,activations_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
